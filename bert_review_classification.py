# -*- coding: utf-8 -*-
"""BERT_review_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r9DoIs48YX1MKKK7B_Xowhkyfr5eDcPK
"""



"""In this notebook, we do amazon review classification with BERT.[Downloaded data from [this](https://www.kaggle.com/snap/amazon-fine-food-reviews/data) link]
<pre> 
It contains 5 parts as below. 
    1. Preprocessing 
    2. Creating a BERT model from the Tensorflow HUB.
    3. Tokenization
    4. getting the pretrained embedding Vector for a given review from the BERT.
    5. Using the embedding data apply NN and classify the reviews.
    6. Creating a Data pipeline for BERT Model. 


</pre>

Here's the [Sequence Diagram](https://www.websequencediagrams.com/cgi-bin/cdraw?lz=dGl0bGUgQmVydCBNb2RlbAoKCgoKCgpSYXcgUmV2aWV3cy0-UHJlcHJvY2Vzc2VkAA4IOiAADAppbmcKCgA5Ci0-AEUKOiBmcm9tIFRGSHViCgoKADYULT5Ub2tlbml6ZWQgdGV4dDogRnVsbCB0AA8GYXRpb24KAEwTRW1iZWRkaW5nczogcG9vbGVkIG91dHB1dAoKCg&s=default) for the below model <br> <br><br>
<img src='https://www.websequencediagrams.com/cgi-bin/cdraw?lz=dGl0bGUgQmVydCBNb2RlbAoKCgoKCgpSYXcgUmV2aWV3cy0-UHJlcHJvY2Vzc2VkAA4IOiAADAppbmcKCgA5Ci0-AEUKOiBmcm9tIFRGSHViCgoKADYULT5Ub2tlbml6ZWQgdGV4dDogRnVsbCB0AA8GYXRpb24KAEwTRW1iZWRkaW5nczogcG9vbGVkIG91dHB1dAAlCAAXCi0-TmV1cmFsIE5ldDogZQAxCCB3b3JkIHZlY3RvcnMKCg&s=default'>

The dataset that we work with is amazon fine food review dataset which contains more than 500k user reviews on food available on amazon.
"""

from google.colab import drive
drive.mount('/content/drive')

"""Getting the data directly into the notebook"""

!wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9" --header="Accept-Language: en-US,en;q=0.9" --header="Referer: https://www.kaggle.com/" "https://storage.googleapis.com/kaggle-data-sets/18/2157/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210920%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210920T011852Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=4cacec106d52a825d3ade8eeb8dc682d25d6dbbaed07acffa35af7f0596c822275a15e824e8232b5a0f6a52d68262202d12c2c3b469d19d302d51f6929a4476e893315193339f12d14ab5f836f17d29cf08b339a30c74e1001c88b614bd7a4592b28333d14d0c363d8ac77fa658d7787c0496148e77616e9b568d138bc7375c2790ef78a86678dd08364c3057a058f0f190fb8148d4f60f7e89b0fe6976f4e760b7705e195cf4a16651e5ffc05df96782944ed102cdf5cc2315d12afe238d766e5d7d555a2b47301fa5431f71ad3b87586e400d980391b6a13016ad75001f5449bc3c51acdbdf8df9478e6a48764e090d16e5266b4c1e2e83086b39db702dfb2" -c -O 'archive.zip'

!unzip archive.zip

#all imports
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import pandas as pd

tf.test.gpu_device_name()

"""<pre><font size=6>Part-1: Preprocessing</font></pre>"""

#Read the dataset - Amazon fine food reviews
reviews = pd.read_csv(r"Reviews.csv")
#check the info of the dataset
reviews.info()

#if score> 3, set score = 1
#if score<=2, set score = 0
#if score == 3, remove the rows.

reviews=reviews[['Text','Score']]

reviews=reviews.loc[reviews.Score!=3]

reviews.Score = reviews.Score.apply(lambda x:1 if x>3 else 0)

reviews.head(5)

def get_wordlen(x):
    return len(x.split())
reviews['len'] = reviews.Text.apply(get_wordlen)
reviews = reviews[reviews.len<50]
reviews = reviews.sample(n=100000, random_state=30)

#remove HTML from the Text column and save in the Text column only

reviews.Text = reviews.Text.replace(r'<[^<>]*>', '', regex=True)

#print head 5

reviews.head()

#split the data into train and test data(20%) with Stratify sampling, random state 33,

X=reviews[['Text','len']]
y=reviews.Score

X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, stratify=y, random_state=30)

#plot bar graphs of y_train and y_test

y_train.value_counts()

print("The ratio of 1 to 0 in y_train is: ",y_train.value_counts()[1]/y_train.value_counts()[0])

y_test.value_counts()

print("The ratio of 1 to 0 in y_train is: ",y_test.value_counts()[1]/y_test.value_counts()[0])

y_train.value_counts().plot(kind="bar")
plt.title("Proportion of 1 vs 0 in y_train")
plt.show()

y_test.value_counts().plot(kind="bar")
plt.title("Proportion of 1 vs 0 in y_train")
plt.show()

#saving to disk. if we need, we can load preprocessed data directly. 
X_train.to_csv('drive/MyDrive/bert_assignment/X_train.csv', index=False)
X_test.to_csv('drive/MyDrive/bert_assignment/X_test.csv', index=False)
y_train.to_csv('drive/MyDrive/bert_assignment/y_train.csv', index=False)
y_test.to_csv('drive/MyDrive/bert_assignment/y_test.csv', index=False)

X_train = pd.read_csv('drive/MyDrive/bert_assignment/X_train.csv')
X_test = pd.read_csv('drive/MyDrive/bert_assignment/X_test.csv')
y_train = pd.read_csv('drive/MyDrive/bert_assignment/y_train.csv')
y_test = pd.read_csv('drive/MyDrive/bert_assignment/y_test.csv')

"""<pre><font size=6>Part-2: Creating BERT Model</font> 


Here we are using <a href="https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1">BERT uncased Base model</a>. 
It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads. </pre>
"""

## Loading the Pretrained Model from tensorflow HUB
tf.keras.backend.clear_session()

# maximum length of a seq in the data we have, for now i am making it as 55. You can change this
max_seq_length = 55

#BERT takes 3 inputs

#this is input words. Sequence of words represented as integers
input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_word_ids")

#mask vector 
input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_mask")

#segment vectors. If we are giving only one sentence for the classification, total seg vector is 0. 
#If we are giving two sentenced with [sep] token separated, first seq segment vectors are zeros and 
#second seq segment vector are 1's
segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="segment_ids")

#bert layer 
bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1", trainable=False)
pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])

#Bert model
#We are using only pooled output not sequence out. 
bert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pooled_output)

bert_model.summary()

bert_model.output

"""<pre><font size=6>Part-3: Tokenization</font></pre>"""

#getting Vocab file
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()

#import tokenization - We have given tokenization.py file

!pip install sentencepiece

import tokenization

# Create tokenizer " Instantiate FullTokenizer" 
# the FullTokenizer takes two parameters 1. vocab_file and 2. do_lower_case 
# we have created these in the above cell ex: FullTokenizer(vocab_file, do_lower_case )

tokenizer = tokenization.FullTokenizer(vocab_file,do_lower_case)

# Create train and test tokens (X_train_tokens, X_test_tokens) from (X_train, X_test) using Tokenizer and 

# add '[CLS]' at start of the Tokens and '[SEP]' at the end of the tokens. 

# maximum number of tokens is 55(We already given this to BERT layer above) so shape is (None, 55)

# if it is less than 55, add '[PAD]' token else truncate the tokens length.(similar to padding)

# Based on padding, create the mask for Train and Test ( 1 for real token, 0 for '[PAD]'), 
# it will also same shape as input tokens (None, 55) save those in X_train_mask, X_test_mask

# Create a segment input for train and test. We are using only one sentence so all zeros. This shape will also (None, 55)

# type of all the above arrays should be numpy arrays

# after execution of this cell, you have to get 
# X_train_tokens, X_train_mask, X_train_segment
# X_test_tokens, X_test_mask, X_test_segment

def tok(token):
  tokens=tokenizer.tokenize(token)[:max_seq_length-2]
  tokens= ['[CLS]',*tokens]
  tokens.extend([*['[PAD]']*(max_seq_length-len(tokens)-1)])
  tokens.append('[SEP]')
  return tokens

X_train_tokens = np.array([tokenizer.convert_tokens_to_ids(i) for i in X_train.Text.apply(tok)])
X_test_tokens = np.array([tokenizer.convert_tokens_to_ids(i) for i in X_test.Text.apply(tok)])
X_train_mask = np.array([np.array(list(map(lambda x:0 if x==0 else 1,X_train_tokens[i]))) for i in range(len(X_train_tokens))])
X_test_mask = np.array([np.array(list(map(lambda x:0 if x==0 else 1,X_test_tokens[i]))) for i in range(len(X_test_tokens))])
X_train_segment=np.repeat(0,max_seq_length*len(X_train)).reshape(-1,max_seq_length)
X_test_segment= np.repeat(0,max_seq_length*len(X_test)).reshape(-1,max_seq_length)

import pickle

##save all results to disk so that, no need to run all again. 
pickle.dump((X_train, X_train_tokens, X_train_mask, X_train_segment, y_train),open('drive/MyDrive/bert_assignment/train_data.pkl','wb'))
pickle.dump((X_test, X_test_tokens, X_test_mask, X_test_segment, y_test),open('drive/MyDrive/bert_assignment/test_data.pkl','wb'))

#load from disk
X_train, X_train_tokens, X_train_mask, X_train_segment, y_train = pickle.load(open("drive/MyDrive/bert_assignment/train_data.pkl", 'rb')) 
X_test, X_test_tokens, X_test_mask, X_test_segment, y_test = pickle.load(open("drive/MyDrive/bert_assignment/test_data.pkl", 'rb'))

"""<pre><font size=6>Part-4: Getting Embeddings from BERT Model</font>
We already created the BERT model in the part-2 and input data in the part-3. 
We will utlize those two and will get the embeddings for each sentence in the 
Train and test data.</pre>
"""

bert_model.input

bert_model.output

# get the train output, BERT model will give one output so save in
# X_train_pooled_output
X_train_pooled_output=bert_model.predict([X_train_tokens,X_train_mask,X_train_segment])

# get the test output, BERT model will give one output so save in
# X_test_pooled_output
X_test_pooled_output=bert_model.predict([X_test_tokens,X_test_mask,X_test_segment])

##save all your results to disk so that, no need to run all again. 
pickle.dump((X_train_pooled_output, X_test_pooled_output),open('drive/MyDrive/bert_assignment/final_output.pkl','wb'))

X_train_pooled_output, X_test_pooled_output= pickle.load(open('drive/MyDrive/bert_assignment/final_output.pkl', 'rb'))

y_train=np.array(y_train).flatten()
y_test=np.array(y_test).flatten()

"""<pre><font size=6>Part-5: Training a NN with 768 features</font>

Creating a NN and training the NN. 
<b> Using AUC as metric.</b> 

</pre>
"""

import tensorflow as tf

##imports
from tensorflow.keras.layers import Input, Dense, Activation, Dropout
from tensorflow.keras.models import Model

inpt = Input(shape=(768,),name='FC')
x=Dense(256,activation='relu',kernel_initializer='he_normal')(inpt)
x=Dropout(0.2)(x)
x=Dense(32,activation='relu',kernel_initializer='he_normal')(x)
otpt = Dense(1,activation='sigmoid')(x)

model= Model(inpt,otpt)
model.summary()

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=[tf.keras.metrics.AUC()]
              )

path='logs/best_model'
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=path,histogram_freq=1,write_images=True)

model.fit(x=X_train_pooled_output,
          y=y_train,
          validation_data=(X_test_pooled_output,y_test),
          epochs=30,
          batch_size=2000,
          callbacks=[tensorboard_callback]
          )

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/best_model

"""<Pre><font size=6>Part-6: Creating a Data pipeline for BERT Model</font> 

1. Downloading test data from <a href="https://drive.google.com/file/d/1QwjqTsqTX2vdy7fTmeXjxP3dq8IAVLpo/view?usp=sharing">here</a>
2. Reading the csv file
3. Removing all the html tags
4. Tokenization
    * Create tokens,mask array and segment array
5. Getting Embeddings from BERT Model , let it be X_test
   * Print the shape of output(X_test.shape).You should get (352,768)
6. Predicting the output of X_test with the Neural network model which we trained earlier.
7. Printing the occurences of class labels in the predicted output

</pre>
"""

!wget --header="Host: doc-0s-b8-docs.googleusercontent.com" --header="User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9" --header="Accept-Language: en-US,en;q=0.9" --header="Referer: https://drive.google.com/" --header="Cookie: AUTH_a7f6fmmlih1nuvpgj1dmasu1pun08n0d_nonce=0cnqjh1l8p8be" --header="Connection: keep-alive" "https://doc-0s-b8-docs.googleusercontent.com/docs/securesc/0krdli91te6eidgb441rq5tt450h33ib/3utuva2ukc8gqcb5tvb5n2n121r2bact/1632101100000/00484516897554883881/01458989393278964861/1QwjqTsqTX2vdy7fTmeXjxP3dq8IAVLpo?e=download&authuser=0&nonce=0cnqjh1l8p8be&user=01458989393278964861&hash=emo0kmh6gseubn3oknfjgaqoqmpg3ugg" -c -O 'test.csv'

def tok(token):
  tokens=tokenizer.tokenize(token)[:max_seq_length-2]
  tokens= ['[CLS]',*tokens]
  tokens.extend([*['[PAD]']*(max_seq_length-len(tokens)-1)])
  tokens.append('[SEP]')
  return tokens

def preprocess_text():
  test.Text = test.Text.replace(r'<[^<>]*>', '', regex=True)
  test_tokens = np.array([tokenizer.convert_tokens_to_ids(i) for i in test.Text.apply(tok)])
  test_mask = np.array([np.array(list(map(lambda x:0 if x==0 else 1,test_tokens[i]))) for i in range(len(test_tokens))])
  test_segment=np.repeat(0,max_seq_length*len(test)).reshape(-1,max_seq_length)
  return test_tokens,test_mask,test_segment

def get_embedding(test_tokens,test_mask,test_segment):
  test_pooled_output=bert_model.predict([test_tokens,test_mask,test_segment])
  return test_pooled_output

def predict_text(embeddings):
  return model.predict(embeddings)

def pipeline():
  test_tokens,test_mask,test_segment = preprocess_text()
  test_pooled_output = get_embedding(test_tokens,test_mask,test_segment)
  print("The shape of the embedding matrix is ",test_pooled_output.shape)
  print("="*50)
  pred = predict_text(test_pooled_output)
  test.insert(1,"Prediction",np.array(list(map(int,pred>0.5))))  
  print("The number of 1 is {} and 0 is {} is test dataset: ".format(test.Prediction.value_counts()[1],test.Prediction.value_counts()[0]))
  print("="*50)
  print("The ratio of 1 to 0 in test dateset is: ",test.Prediction.value_counts()[1]/test.Prediction.value_counts()[0])
  print("="*50)
  test.Prediction.value_counts().plot(kind="bar")
  plt.title("Proportion of 1 vs 0 in test dateset")
  plt.show()
  print("="*50)
  print("The predictions are: \n",test)

test=pd.read_csv('test.csv')

pipeline()

"""#Observation and Conclusion

In the above experimentation, we first get the Amazon fine food review. Only 100k reviews are sampled out of the available 520k reviews. The scores provided range anywhere from 1 to 5, so we discard the reviews with score 3 since 3 is the mean score and doesn't contribute much to the classification of reviews. The scores less than 3 are classified as 0 and the scores more than 3 are classified as 1, 0 meaning neagative review, 1 being positive. Effectively out task is transformed into binary clasification probelem from a multi class classification problem.
Only the review texts and the scores are kept and the other columns are discarded since the goal in here is to use BERT word embedding only to classify the review.

The sentences are tokenised. Before they are tokenized, the HTML tags are removed since they don't contribute to meaning of the sentence. The punctatuions are kept since punctuations from a large part of the underlying meaning of the sentence. The tokenised sentence or the token vector is then appended with '[CLS]' at the beginning and '[SEP]' at the end to mark to the bert model beginning and separation of sentences respectively.

The bert model is built for standard sentence classification, so we create three inputs, namely the tokens, the masks vector, and the segments vector. Since the tokens are padded to match the length equal to the maximum sequence length, we take every token vector and transform it into length equal to max_sequence_length by pading '[PAD]' to the end. the mask vector indicates 0 if theres a pad word at corresponging position in the token vector, 1 otherwise. 

Three inputs are sent into the bert model as mentioned above and one output. We are taking the pooled output which means we get embedding for every sentence but not for each word. Then, a neural network with sigmoid classification is trained on the embedding obtained from the model. 

An AUC of more than 0.95 is achieved on the validation results and the same models (both BERT and Neural Network) are used to classify the test data that os provided. The ratio of negative and positive points of the test dataset and the train dataset is nearly the same.

From this, it can be concluded the BERT model for sentence embedding provides very accurate embedding of the input sentence and fine tuning the embeddings a little bit can produce state of the art results for particular nlp tasks.
"""